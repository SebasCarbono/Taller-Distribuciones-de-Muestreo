# -*- coding: utf-8 -*-
"""Taller_Distribuciones_Muestreo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RAAcT5t-hcuSql4eV_zeXv9X3IrXL4Pi
"""

# Si están trabajando en Google Colab, no es necesario instalar estas librerías.
# Si usan un entorno local, pueden descomentar e instalar:
!pip install numpy pandas matplotlib scipy seaborn

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import expon, uniform, norm


sns.set(style="whitegrid")

# Parámetros para n = 10
n_samples_10 = 10000  # número de muestras
n_10 = 10             # tamaño de cada muestra

# Distribuciones para n = 10
exp_data_10 = stats.expon.rvs(scale=10, size=(n_10, n_samples_10))     # λ = 1/10 → scale = 10
uni_data_10 = stats.uniform.rvs(loc=5, scale=10, size=(n_10, n_samples_10))  # a=5, b=15 → scale=b-a
norm_data_10 = stats.norm.rvs(loc=10, scale=2.5, size=(n_10, n_samples_10))  # μ=10, σ=2.5

# Crear DataFrames para n = 10
df_exp_10 = pd.DataFrame(exp_data_10)
df_uni_10 = pd.DataFrame(uni_data_10)
df_norm_10 = pd.DataFrame(norm_data_10)

df_exp_10.head()

# Parámetros para n = 20
n_samples_20 = 10000  # número de muestras
n_20 = 20             # tamaño de cada muestra

# Distribuciones para n = 20
exp_data_20 = stats.expon.rvs(scale=10, size=(n_20, n_samples_20))     # λ = 1/10 → scale = 10
uni_data_20 = stats.uniform.rvs(loc=5, scale=10, size=(n_20, n_samples_20))  # a=5, b=15 → scale=b-a
norm_data_20 = stats.norm.rvs(loc=10, scale=2.5, size=(n_20, n_samples_20))  # μ=10, σ=2.5

# Crear DataFrames para n = 20
df_exp_20 = pd.DataFrame(exp_data_20)
df_uni_20 = pd.DataFrame(uni_data_20)
df_norm_20 = pd.DataFrame(norm_data_20)

df_exp_20.head()

# Parámetros para n = 50
n_samples_50 = 10000  # número de muestras
n_50 = 50             # tamaño de cada muestra

# Distribuciones para n = 50
exp_data_50 = stats.expon.rvs(scale=10, size=(n_50, n_samples_50))     # λ = 1/10 → scale = 10
uni_data_50 = stats.uniform.rvs(loc=5, scale=10, size=(n_50, n_samples_50))  # a=5, b=15 → scale=b-a
norm_data_50 = stats.norm.rvs(loc=10, scale=2.5, size=(n_50, n_samples_50))  # μ=10, σ=2.5

# Crear DataFrames para n = 50
df_exp_50 = pd.DataFrame(exp_data_50)
df_uni_50 = pd.DataFrame(uni_data_50)
df_norm_50 = pd.DataFrame(norm_data_50)

df_exp_50.head()

# Exportar un subconjunto para evitar archivos grandes en Colab
df_exp_10.to_csv("expon_n10.csv", index=False)
df_uni_10.to_csv("uniform_n10.csv", index=False)
df_norm_10.to_csv("norm_n10.csv", index=False)

df_exp_20.to_csv("expon_n20.csv", index=False)
df_uni_20.to_csv("uniform_n20.csv", index=False)
df_norm_20.to_csv("norm_n20.csv", index=False)

df_exp_50.to_csv("expon_n50.csv", index=False)
df_uni_50.to_csv("uniform_n50.csv", index=False)
df_norm_50.to_csv("norm_n50.csv", index=False)

# --- CONSOLIDACIÓN DE DATOS ---

# Para cada distribución unir n=10, n=20, n=50 en una sola columna
exp_consolidated = pd.concat([df_exp_10.stack(),
                              df_exp_20.stack(),
                              df_exp_50.stack()],
                             ignore_index=True)

uni_consolidated = pd.concat([df_uni_10.stack(),
                              df_uni_20.stack(),
                              df_uni_50.stack()],
                             ignore_index=True)

norm_consolidated = pd.concat([df_norm_10.stack(),
                               df_norm_20.stack(),
                               df_norm_50.stack()],
                              ignore_index=True)

# Crear el DataFrame final
df_consolidated = pd.DataFrame({
    "Exponencial": exp_consolidated,
    "Uniforme": uni_consolidated,
    "Normal": norm_consolidated
})

# Exportar
df_consolidated.to_csv("datos.csv", index=False)

print("Archivo datos.csv generado con éxito. Dimensiones:")
print(df_consolidated.shape)

# --- ANALISIS DESCRIPTIVO ---

print("ESTADÍSTICOS DESCRIPTIVOS\n")

desc_stats = df_consolidated.describe()
print(desc_stats)

print("\nSesgo (skewness):")
print(df_consolidated.skew())

print("\nApuntamiento (kurtosis):")
print(df_consolidated.kurtosis())

fig, axes = plt.subplots(1, 3, figsize=(15, 4))

columnas = ["Exponencial", "Uniforme", "Normal"]

for ax, col in zip(axes, columnas):

    data = df_consolidated[col]

    # Histograma
    ax.hist(data, bins='sturges', density=True, alpha=0.6, color='steelblue')

    # Modelo teórico
    x = np.linspace(data.min(), data.max(), 300)

    if col == "Exponencial":
        pdf = expon(scale=10).pdf(x)
    elif col == "Uniforme":
        pdf = uniform(loc=5, scale=10).pdf(x)
    else:  # Normal
        pdf = norm(loc=10, scale=2.5).pdf(x)

    ax.plot(x, pdf, 'r', linewidth=2)

    ax.set_title(f"{col}: Histograma + Modelo Teórico")
    ax.set_xlabel("Valor")
    ax.set_ylabel("Frecuencia relativa")

plt.tight_layout()
plt.show()

# Calcular estadísticos básicos para n = 10
summary_10 = pd.DataFrame({
    "Exponencial": df_exp_10.stack().describe(),
    "Uniforme": df_uni_10.stack().describe(),
    "Normal": df_norm_10.stack().describe()
})
summary_10

# Calcular estadísticos básicos para n = 20
summary_20 = pd.DataFrame({
    "Exponencial": df_exp_20.stack().describe(),
    "Uniforme": df_uni_20.stack().describe(),
    "Normal": df_norm_20.stack().describe()
})
summary_20

# Calcular estadísticos básicos para n = 50
summary_50 = pd.DataFrame({
    "Exponencial": df_exp_50.stack().describe(),
    "Uniforme": df_uni_50.stack().describe(),
    "Normal": df_norm_50.stack().describe()
})
summary_50

# Histograma comparativo para n = 10 (Ley de Sturges)
plt.figure(figsize=(10,6))
plt.hist(df_exp_10.values.flatten(), bins='sturges', alpha=0.6, density=True, label="Exponencial")
plt.hist(df_uni_10.values.flatten(), bins='sturges', alpha=0.6, density=True, label="Uniforme")
plt.hist(df_norm_10.values.flatten(), bins='sturges', alpha=0.6, density=True, label="Normal")
plt.title("Distribuciones poblacionales generadas (bins = Sturges, n = 10)")
plt.xlabel("Valor")
plt.ylabel("Frecuencia relativa")
plt.legend()
plt.show()

# Histograma comparativo para n = 20 (Ley de Sturges)
plt.figure(figsize=(10,6))
plt.hist(df_exp_20.values.flatten(), bins='sturges', alpha=0.6, density=True, label="Exponencial")
plt.hist(df_uni_20.values.flatten(), bins='sturges', alpha=0.6, density=True, label="Uniforme")
plt.hist(df_norm_20.values.flatten(), bins='sturges', alpha=0.6, density=True, label="Normal")
plt.title("Distribuciones poblacionales generadas (bins = Sturges, n = 20)")
plt.xlabel("Valor")
plt.ylabel("Frecuencia relativa")
plt.legend()
plt.show()

# Histograma comparativo para n = 50 (Ley de Sturges)
plt.figure(figsize=(10,6))
plt.hist(df_exp_50.values.flatten(), bins='sturges', alpha=0.6, density=True, label="Exponencial")
plt.hist(df_uni_50.values.flatten(), bins='sturges', alpha=0.6, density=True, label="Uniforme")
plt.hist(df_norm_50.values.flatten(), bins='sturges', alpha=0.6, density=True, label="Normal")
plt.title("Distribuciones poblacionales generadas (bins = Sturges, n = 50)")
plt.xlabel("Valor")
plt.ylabel("Frecuencia relativa")
plt.legend()
plt.show()

# Calcular las medias de cada muestra para n = 10 (por columnas)
means_exp_10 = df_exp_10.mean(axis=0)
means_uni_10 = df_uni_10.mean(axis=0)
means_norm_10 = df_norm_10.mean(axis=0)

# Crear DataFrame resumen
df_means_10 = pd.DataFrame({
    "Exponencial (n = 10)": means_exp_10,
    "Uniforme (n = 10)": means_uni_10,
    "Normal (n = 10)": means_norm_10
})
df_means_10.describe()

# Calcular las medias de cada muestra para n = 20 (por columnas)
means_exp_20 = df_exp_20.mean(axis=0)
means_uni_20 = df_uni_20.mean(axis=0)
means_norm_20 = df_norm_20.mean(axis=0)

# Crear DataFrame resumen
df_means_20 = pd.DataFrame({
    "Exponencial (n = 20)": means_exp_20,
    "Uniforme (n = 20)": means_uni_20,
    "Normal (n = 20)": means_norm_20
})
df_means_20.describe()

# Calcular las medias de cada muestra para n = 50 (por columnas)
means_exp_50 = df_exp_50.mean(axis=0)
means_uni_50 = df_uni_50.mean(axis=0)
means_norm_50 = df_norm_50.mean(axis=0)

# Crear DataFrame resumen
df_means_50 = pd.DataFrame({
    "Exponencial (n = 50)": means_exp_50,
    "Uniforme (n = 50)": means_uni_50,
    "Normal (n = 50)": means_norm_50
})
df_means_50.describe()

# --- REQUISITO 5: DATAFRAMES DE MEDIAS MUESTRALES ---

# 1. Medias para la distribución EXPONENCIAL
means_exp_10 = df_exp_10.mean(axis=0)
means_exp_20 = df_exp_20.mean(axis=0)
means_exp_50 = df_exp_50.mean(axis=0)

df_means_exponencial = pd.DataFrame({
    "media_n10": means_exp_10,
    "media_n20": means_exp_20,
    "media_n50": means_exp_50
})

print("Medias muestrales - Exponencial (primeras filas):")
print(df_means_exponencial.head())


# 2. Medias para la distribución UNIFORME
means_uni_10 = df_uni_10.mean(axis=0)
means_uni_20 = df_uni_20.mean(axis=0)
means_uni_50 = df_uni_50.mean(axis=0)

df_means_uniforme = pd.DataFrame({
    "media_n10": means_uni_10,
    "media_n20": means_uni_20,
    "media_n50": means_uni_50
})

print("\nMedias muestrales - Uniforme (primeras filas):")
print(df_means_uniforme.head())


# 3. Medias para la distribución NORMAL
means_norm_10 = df_norm_10.mean(axis=0)
means_norm_20 = df_norm_20.mean(axis=0)
means_norm_50 = df_norm_50.mean(axis=0)

df_means_normal = pd.DataFrame({
    "media_n10": means_norm_10,
    "media_n20": means_norm_20,
    "media_n50": means_norm_50
})

print("\nMedias muestrales - Normal (primeras filas):")
print(df_means_normal.head())

# Visualización del Teorema del Límite Central para n = 10 (bins = Sturges)
fig, axes = plt.subplots(1, 3, figsize=(14, 4))
distros = ["Exponencial (n = 10)", "Uniforme (n = 10)", "Normal (n = 10)"]

for i, dist in enumerate(distros):
    axes[i].hist(df_means_10[dist], bins='sturges', alpha=0.7, density=True)
    axes[i].set_title(f"Medias muestrales - {dist}")
    axes[i].set_xlabel("Media muestral")
    axes[i].set_ylabel("Frecuencia relativa")

plt.suptitle("Distribución de las medias muestrales para n = 10 (bins = Sturges)")
plt.show()

# Visualización del Teorema del Límite Central para n = 20 (bins = Sturges)
fig, axes = plt.subplots(1, 3, figsize=(14, 4))
distros = ["Exponencial (n = 20)", "Uniforme (n = 20)", "Normal (n = 20)"]

for i, dist in enumerate(distros):
    axes[i].hist(df_means_20[dist], bins='sturges', alpha=0.7, density=True)
    axes[i].set_title(f"Medias muestrales - {dist}")
    axes[i].set_xlabel("Media muestral")
    axes[i].set_ylabel("Frecuencia relativa")

plt.suptitle("Distribución de las medias muestrales para n = 20 (bins = Sturges)")
plt.show()

# Visualización del Teorema del Límite Central para n = 50 (bins = Sturges)
fig, axes = plt.subplots(1, 3, figsize=(14, 4))
distros = ["Exponencial (n = 50)", "Uniforme (n = 50)", "Normal (n = 50)"]

for i, dist in enumerate(distros):
    axes[i].hist(df_means_50[dist], bins='sturges', alpha=0.7, density=True)
    axes[i].set_title(f"Medias muestrales - {dist}")
    axes[i].set_xlabel("Media muestral")
    axes[i].set_ylabel("Frecuencia relativa")

plt.suptitle("Distribución de las medias muestrales para n = 50 (bins = Sturges)")
plt.show()

# --- REQUISITO 6: 9 HISTOGRAMAS EN UNA SOLA FIGURA ---

# Diccionario para iterar más fácilmente
distribuciones = {
    "Exponencial": df_means_exponencial,
    "Uniforme": df_means_uniforme,
    "Normal": df_means_normal
}

tamanos = ["media_n10", "media_n20", "media_n50"]

fig, axes = plt.subplots(3, 3, figsize=(14, 12))

for i, (nombre_dist, df_means) in enumerate(distribuciones.items()):
    for j, col in enumerate(tamanos):
        ax = axes[i][j]

        # Histograma con frecuencia relativa
        ax.hist(df_means[col], bins="sturges", density=True, alpha=0.7, color="steelblue")

        ax.set_title(f"{nombre_dist} - {col}")
        ax.set_xlabel("Media muestral")
        ax.set_ylabel("Frecuencia relativa")

plt.suptitle("Distribución de las medias muestrales\n(3 distribuciones × 3 tamaños)", fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.97])  # Ajusta espacio para el título
plt.show()

def merge_small_expected(freqs, expected, min_exp=5.0):
    freqs = freqs.astype(float).tolist()
    expected = expected.astype(float).tolist()
    i = 0
    while i < len(expected):
        if expected[i] < min_exp:
            if i == 0 and len(expected) > 1:
                expected[i+1] += expected[i]; freqs[i+1] += freqs[i]
                del expected[i]; del freqs[i]
            else:
                expected[i-1] += expected[i]; freqs[i-1] += freqs[i]
                del expected[i]; del freqs[i]
                i -= 1
        else:
            i += 1
    return np.array(freqs), np.array(expected)

def chi_square_normal_test(data):
    data = np.asarray(data)

    # Estimar parámetros de normal
    mu = data.mean()
    sig = data.std(ddof=1)

    # Histograma
    freqs, bins = np.histogram(data, bins="sturges")
    probs = stats.norm.cdf(bins[1:], loc=mu, scale=sig) - stats.norm.cdf(bins[:-1], loc=mu, scale=sig)

    expected = probs * freqs.sum()
    expected *= (freqs.sum() / expected.sum())  # reescalar

    # Fusionar clases pequeñas
    freqs_adj, expected_adj = merge_small_expected(freqs, expected)
    expected_adj *= (freqs_adj.sum() / expected_adj.sum())

    # Chi-cuadrado
    chi2_stat, chi2_p = stats.chisquare(f_obs=freqs_adj, f_exp=expected_adj, ddof=2)
    return chi2_stat, chi2_p

def run_normality_tests(name, df):
  print("\n-------------------------------------------")
  print(f"PRUEBAS DE NORMALIDAD - {name}")
  print("-------------------------------------------")

  for col in df.columns:
      data = df[col]

      # Shapiro (usar muestra aleatoria de 500 para eficiencia)
      sample = data.sample(500, random_state=42)
      shapiro_stat, shapiro_p = stats.shapiro(sample)

      # Chi-cuadrado
      chi2_stat, chi2_p = chi_square_normal_test(data)

      print(f"\nTamaño de muestra: {col}")
      print(f"  Shapiro-Wilk: estadístico = {shapiro_stat:.4f}, p-valor = {shapiro_p:.4f}")
      print(f"  Chi-cuadrado: estadístico = {chi2_stat:.4f}, p-valor = {chi2_p:.4f}")

      if shapiro_p > 0.05:
          print("  → Shapiro: NO se rechaza normalidad")
      else:
          print("  → Shapiro: Se rechaza normalidad")

      if chi2_p > 0.05:
          print("  → Chi-cuadrado: NO se rechaza normalidad")
      else:
          print("  → Chi-cuadrado: Se rechaza normalidad")

run_normality_tests("Distribución Exponencial", df_means_exponencial)
run_normality_tests("Distribución Uniforme", df_means_uniforme)
run_normality_tests("Distribución Normal", df_means_normal)

# --- REQUISITO 8: COMPARACIÓN DE POTENCIA ENTRE SHAPIRO Y CHI-CUADRADO ---

n_rep = 1000  # repeticiones como exige el PDF
n_data = 10000  # tamaño de la muestra binomial
rng = np.random.default_rng(123)  # semilla

rej_shapiro = 0
rej_chi2 = 0

for _ in range(n_rep):
    # 1. Generar datos binomiales
    x = stats.binom.rvs(n=100, p=0.30, size=n_data, random_state=rng.integers(1,1_000_000))

    # 2. Shapiro-Wilk (usar muestra de 500 por eficiencia)
    sample = np.random.choice(x, 500, replace=False)
    _, p_shapiro = stats.shapiro(sample)
    if p_shapiro <= 0.05:
        rej_shapiro += 1

    # 3. Chi-cuadrado
    _, p_chi2 = chi_square_normal_test(x)
    if p_chi2 <= 0.05:
        rej_chi2 += 1

print("\nPOTENCIA EXPERIMENTAL (rechazo de H0 de normalidad):")
print(f"Shapiro-Wilk: {rej_shapiro / n_rep:.3f}")
print(f"Chi-cuadrado: {rej_chi2 / n_rep:.3f}")

# Pruebas de normalidad y bondad de ajuste para n = 10
sample_10 = df_means_10["Exponencial (n = 10)"]

# Shapiro-Wilk (submuestra por eficiencia)
stat_shapiro, p_shapiro = stats.shapiro(sample_10.sample(500, random_state=42))
print(f"Shapiro-Wilk: estadístico={stat_shapiro:.4f}, p-valor={p_shapiro:.4f}")

# Chi-cuadrado con reescalado y fusión de bins de baja frecuencia (bins = Sturges)
def merge_small_expected(freqs, expected, min_exp=5.0):
    freqs = freqs.astype(float).tolist()
    expected = expected.astype(float).tolist()
    i = 0
    while i < len(expected):
        if expected[i] < min_exp:
            if i == 0 and len(expected) > 1:
                expected[i+1] += expected[i]; freqs[i+1] += freqs[i]
                del expected[i]; del freqs[i]
            else:
                expected[i-1] += expected[i]; freqs[i-1] += freqs[i]
                del expected[i]; del freqs[i]
                i -= 1
        else:
            i += 1
    return np.array(freqs), np.array(expected)

mu = np.mean(sample_10)
sig = np.std(sample_10, ddof=1)

# Bins de Sturges
freqs, bins = np.histogram(sample_10, bins='sturges')
probs = stats.norm.cdf(bins[1:], loc=mu, scale=sig) - stats.norm.cdf(bins[:-1], loc=mu, scale=sig)
expected = probs * freqs.sum()
expected *= (freqs.sum() / expected.sum())

freqs_adj, expected_adj = merge_small_expected(freqs, expected, min_exp=5.0)
expected_adj *= (freqs_adj.sum() / expected_adj.sum())

chi2_stat, chi2_p = stats.chisquare(f_obs=freqs_adj, f_exp=expected_adj, ddof=2)
print(f"Chi-cuadrado: estadístico={chi2_stat:.4f}, p-valor={chi2_p:.4f}")

# Pruebas de normalidad y bondad de ajuste para n = 20
sample_20 = df_means_20["Exponencial (n = 20)"]

# Shapiro-Wilk (submuestra por eficiencia)
stat_shapiro, p_shapiro = stats.shapiro(sample_20.sample(500, random_state=42))
print(f"Shapiro-Wilk: estadístico={stat_shapiro:.4f}, p-valor={p_shapiro:.4f}")

# Chi-cuadrado con reescalado y fusión de bins de baja frecuencia (bins = Sturges)
def merge_small_expected(freqs, expected, min_exp=5.0):
    freqs = freqs.astype(float).tolist()
    expected = expected.astype(float).tolist()
    i = 0
    while i < len(expected):
        if expected[i] < min_exp:
            if i == 0 and len(expected) > 1:
                expected[i+1] += expected[i]; freqs[i+1] += freqs[i]
                del expected[i]; del freqs[i]
            else:
                expected[i-1] += expected[i]; freqs[i-1] += freqs[i]
                del expected[i]; del freqs[i]
                i -= 1
        else:
            i += 1
    return np.array(freqs), np.array(expected)

mu = np.mean(sample_20)
sig = np.std(sample_20, ddof=1)

# Bins de Sturges
freqs, bins = np.histogram(sample_20, bins='sturges')
probs = stats.norm.cdf(bins[1:], loc=mu, scale=sig) - stats.norm.cdf(bins[:-1], loc=mu, scale=sig)
expected = probs * freqs.sum()
expected *= (freqs.sum() / expected.sum())

freqs_adj, expected_adj = merge_small_expected(freqs, expected, min_exp=5.0)
expected_adj *= (freqs_adj.sum() / expected_adj.sum())

chi2_stat, chi2_p = stats.chisquare(f_obs=freqs_adj, f_exp=expected_adj, ddof=2)
print(f"Chi-cuadrado: estadístico={chi2_stat:.4f}, p-valor={chi2_p:.4f}")

# Pruebas de normalidad y bondad de ajuste para n = 50
sample_50 = df_means_50["Exponencial (n = 50)"]

# Shapiro-Wilk (submuestra por eficiencia)
stat_shapiro, p_shapiro = stats.shapiro(sample_50.sample(500, random_state=42))
print(f"Shapiro-Wilk: estadístico={stat_shapiro:.4f}, p-valor={p_shapiro:.4f}")

# Chi-cuadrado con reescalado y fusión de bins de baja frecuencia (bins = Sturges)
def merge_small_expected(freqs, expected, min_exp=5.0):
    freqs = freqs.astype(float).tolist()
    expected = expected.astype(float).tolist()
    i = 0
    while i < len(expected):
        if expected[i] < min_exp:
            if i == 0 and len(expected) > 1:
                expected[i+1] += expected[i]; freqs[i+1] += freqs[i]
                del expected[i]; del freqs[i]
            else:
                expected[i-1] += expected[i]; freqs[i-1] += freqs[i]
                del expected[i]; del freqs[i]
                i -= 1
        else:
            i += 1
    return np.array(freqs), np.array(expected)

mu = np.mean(sample_50)
sig = np.std(sample_50, ddof=1)

# Bins de Sturges
freqs, bins = np.histogram(sample_50, bins='sturges')
probs = stats.norm.cdf(bins[1:], loc=mu, scale=sig) - stats.norm.cdf(bins[:-1], loc=mu, scale=sig)
expected = probs * freqs.sum()
expected *= (freqs.sum() / expected.sum())

freqs_adj, expected_adj = merge_small_expected(freqs, expected, min_exp=5.0)
expected_adj *= (freqs_adj.sum() / expected_adj.sum())

chi2_stat, chi2_p = stats.chisquare(f_obs=freqs_adj, f_exp=expected_adj, ddof=2)
print(f"Chi-cuadrado: estadístico={chi2_stat:.4f}, p-valor={chi2_p:.4f}")

"""**Interpretación:**
- Si el valor p > 0.05 → No se rechaza la hipótesis nula de normalidad.
- Si el valor p ≤ 0.05 → Se rechaza la hipótesis de normalidad.
"""

# Comparación de potencia entre Shapiro y Chi-cuadrado (demostración simplificada)
n_rep = 200
rej_shapiro, rej_chi2 = 0, 0

def chisq_normal_gof(x):
    x = np.asarray(x)
    mu = x.mean(); sig = x.std(ddof=1)
    # Bins = Sturges dentro del rango observado
    f_obs, bins = np.histogram(x, bins='sturges')
    probs = stats.norm.cdf(bins[1:], loc=mu, scale=sig) - stats.norm.cdf(bins[:-1], loc=mu, scale=sig)
    f_exp = probs * f_obs.sum()
    if f_exp.sum() <= 0:
        return np.nan, np.nan
    # Reescalar para igualdad exacta de sumas
    f_exp *= (f_obs.sum() / f_exp.sum())
    # Fusionar bins con esperadas < 5
    def merge_small(freqs, expected, min_exp=5.0):
        freqs = freqs.astype(float).tolist(); expected = expected.astype(float).tolist()
        i = 0
        while i < len(expected):
            if expected[i] < min_exp:
                if i == 0 and len(expected) > 1:
                    expected[i+1] += expected[i]; freqs[i+1] += freqs[i]
                    del expected[i]; del freqs[i]
                else:
                    expected[i-1] += expected[i]; freqs[i-1] += freqs[i]
                    del expected[i]; del freqs[i]
                    i -= 1
            else:
                i += 1
        return np.array(freqs), np.array(expected)
    f_obs2, f_exp2 = merge_small(f_obs, f_exp, min_exp=5.0)
    f_exp2 *= (f_obs2.sum() / f_exp2.sum())
    chi2_stat, chi2_p = stats.chisquare(f_obs=f_obs2, f_exp=f_exp2, ddof=2)
    return chi2_stat, chi2_p

rng = np.random.default_rng(123)
for _ in range(n_rep):
    x = stats.binom.rvs(n=100, p=0.3, size=1000, random_state=rng.integers(0, 1_000_000))
    # Shapiro (submuestra para velocidad)
    _, p_s = stats.shapiro(np.random.choice(x, 500, replace=False))
    if p_s <= 0.05:
        rej_shapiro += 1
    # Chi-cuadrado robusto (bins = Sturges)
    _, p_c = chisq_normal_gof(x)
    if np.isfinite(p_c) and p_c <= 0.05:
        rej_chi2 += 1

print("Potencia experimental (rechazo H0 de normalidad):")
print(f"Shapiro-Wilk: {rej_shapiro/n_rep:.2f}")
print(f"Chi-cuadrado: {rej_chi2/n_rep:.2f}")